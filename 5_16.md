## Pytorch

vision_datasets(
	root = root, 
	train = True, 
	transform T.ToTensor(), 
	download= download
)



- transform, ToTensor() makes the image fils as tensor.

  download makes datasets be downloaded.

- If the datasets are test datasets, then train = False

- Numworker = 1 or 4 or 8 -> Meaning the number of cores that will be used in training.

-  



X = x.view(-1, 28, 28) # x.view convert the shape of tensor, (Batch_size, 28, 28) -> (Batch_size, 28 * 28)
-1 means that it accepts anything given.



- Activation Map ~= Feature Map

- Stride and padding could also expressed as = (1,1) or (2,2) 

- [Original image one-side dimension - kernel size + 2 * (padding size) / stride ]내림 + 1

- Is fully connected layer's bias 1 or b?



Normalizing prevents certain value in the certain channel from diverging so that it doesn't affect calculation too much to the other operation.



Why VGG uses 3X3 filter instead of 7X7 filters?

- It can use much less parameters
- It can use more activation functions by stacking more layers, then it means it has more non-linearity.



### <br>Receptive Field

Even though GPU or computing power is lack, the computer scientist recommends that stacking layers more than 3(for example). Because for example only having three 3 X 3 filters, then it means that the subdomain area only implies small subdomain. However, if network has six 3 X 3 filters, then it means that each subdomain could imply bigger part of the image.
